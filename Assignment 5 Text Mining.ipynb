{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Dates Extraction Using Regular Expressions\n",
    "\n",
    "In part 1, you'll be working with messy medical data and using regex to extract relevant infromation from the data. \n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.\n",
    "\n",
    "The goal of this assignment is to extract all of the different date variants encoded in this dataset.\n",
    "\n",
    "Here is a list of some of the variants you might encounter in this dataset:\n",
    "* 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009;\n",
    "* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "* Feb 2009; Sep 2009; Oct 2010\n",
    "* 6/2008; 12/2009\n",
    "* 2009; 2010\n",
    "\n",
    "*This function should return a Series of all the dates in dates.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         03/25/93 Total time of visit (in minutes):\\n\n",
       "1                       6/18/85 Primary Care Doctor:\\n\n",
       "2    sshe plans to move as of 7/8/71 In-Home Servic...\n",
       "3                7 on 9/27/75 Audit C Score Current:\\n\n",
       "4    2/6/96 sleep studyPain Treatment Pain Level (N...\n",
       "5                    .Per 7/06/79 Movement D/O note:\\n\n",
       "6    4, 5/18/78 Patient's thoughts about current su...\n",
       "7    10/24/89 CPT Code: 90801 - Psychiatric Diagnos...\n",
       "8                         3/7/86 SOS-10 Total Score:\\n\n",
       "9             (4/10/71)Score-1Audit C Score Current:\\n\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Brian Conway\n",
    "#CS 411 assignment 5\n",
    "#Prof Kamile Samara, Spring 2021\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6/18/8', '9/27/7', 'Per 7', '10/24/8', '3/7/8', '5/11/8', '8/01/9', '5/24/1', '1/25/2', '10/13/1', '5/21/7', '10/21/7', '9/02/7', '10/24/8', '7/20/7', '4/12/8', '06/20/9', '3/15/8', '5/24/8', '7/27/1', '1-14-81', '8/14/9', '8/16/8', '2/15/1', '7/15/9', '06/12/9', '2/28/7', '5/24/9', '12/01/7', '12/5/2', '08/20/1', '8/06/8', '02/22/9', '6/28/8', '9920', '7/6/9', '11/3/1', 'Lithium 0', '7/11/7', '9/36/3', '09/19/8', '9/6/7', '01/05/1', '10/04/9', '8/04/7', '09/14/2', '8/09/1', '8/9/9', '8/26/8', '10/13/9', '04/08/2', '4/11/1', '3/31/9', '011/14/8', '10/05/9', '07/18/2', '9/22/8', '2/24/7', '2/11/2', '8/22/8', '6/17/9', '10/16/8', '4/05/8', '6/20/7', '07/17/9', '12/22/9', '10/02/9', '11/05/9', '5/04/7', 'Jan 2', 'Sep 2', 'May 1', 'June 2', 'May 1', 'Oct 1', 'Oct 1', 'Nov 2', 'June 1', 'Jan 1', 'Oct 1', 'February 1', 'Feb 1', 'Feb 1', 'Oct 2', 'Feb 1', 'May 2', 'Jan 1', 'Oct 1', 'Oct 2', 'Oct 1', 'Nov 2', 'May 2', 'Feb 1', 'Sep 1', 'March 1', 'June 1', 'Sep 2', 'Jan 1', 'Mar 1', 'Oct 1', 'May 1', 'Feb 1', 'Aug 2', 'May 2', 'Oct 2', 'Oct 2', 'Mar 1', 'Jan 1', 'Oct 1', 'August 1', 'Nov 1', 'Oct 1', 'Oct 1', 'Oct 1', 'Jan 2', 'Oct 1', 'Aug 1', 'Oct 2', 'Dec 1', 'Oct 1', 'May 2', 'Jan 1', 'Jun 1', 'Dec 1', 'Dec 1', 'August 1', 'June 1', 'May 2', 'Nov 2', 'Aug 1', 'Oct 2', 'Jan 2', 'March 2', 'Oct 1', 'Aug 2', 'Nov 1', 'May 1', 'Jan 1', 'April 1', 'May 3', 'Feb 1', 'February 1', 'Jan 2', 'July 2', 'December 2', 'May 1', 'September 0', '1976', 'Jan 2', 'October 2', 'August 1', 'September 0', 'July 2', '1983', 'August 1', 'April 1', 'July 2', 'July 1', '9079', 'August 1', 'Nov 1', 'June 1', 'May 1', 'Dec 1', 'June 2', 'Oct 1', 'May 1', 'October 1', 'July 2', '9079', 'June 1', 'January 0', '2011', 'September 1', 'June 2', 'May 1', 'May 2', 'July 1', 'Jul 2', 'Oct 2', 'Wellbutrin 1', 'May 1', 'February 1', 'January 1', 'Feb 1', '2011', 'May 2', 'Nov 2', 'Sep 2', '2013', 'November 1', 'July 1', 'May 1', 'July 1', 'April 1', 'May 2', 'December 1', 'Jan 2', 'Feb 2', 'August 1', 'Oct 2', 'Aug 1', 'Sep 2', 'Apr 1', 'Nov 1', 'February 2', 'Oct 1', 'Jun 2', 'September 1', 'June 2', 'April 1', 'September 1', 'Oct 1', 'Dec 2', 'July 1', 'Ely 7', 'August 2', 'Feb 1', 'Zoloft 1', '2010', 'April 1', 'September 1', 'April 1', 'Apr 2', 'September 1', 'Sep 2', 'July 1', 'Aug 2', 'May 2', 'Feb 1', 'Jan 1', 'Sep 1', 'January 2', 'Mar 2', 'August 2', 'Sep 2', 'December 2', 'Jan 2', 'November 1', 'September 2', 'February 1', 'March 1', 'Aug 1', 'Jan 2', 'Janaury 1', 'March 1', 'January 1', 'Dec 1', 'November 2', 'January 1', 'Mar 2', 'Feb 2', 'July 2', 'Feb 1', 'April 1', 'Oct 1', 'February 1', 'Decemeber 1', 'January 2', 'Jun 1', 'May 2', 'Jan 1', 'July 1', 'November 2', 'October 1', 'March 1', 'October 1', 'Jun 2', 'October 1', 'April 1', '2000', 'April 1', 'December 1', 'June 1', 'November 1', 'July 1', 'February 1', 'Take 1', 'March 1', 'Dec 2', '2005', 'May 1', 'Nov 2', 'March 1', '6/2005', '9/2005', '12/2005', '5/2004', '3/1986', '3/1993', '9/2003', '1/1983', '12/2008', '8/2003', '11/2010', '7/1997', '9/2001', '1/1978', '1/2009', '8/1989', '06/1973', '6/2001', '8/2009', '1/2014', '12/2012', '7/1989', '11/1998', '2/2009', '4/2007', '4/2012', '2/1977', '2/1983', '1/1992', '4/1974', '10/1986', '12/1994', '10/2010', '01/2007', '03/1990', '4/2004', 'Hospital 7', '8/2000', '1975', '3/2000', '12/1986', '7/2004', '5/2016', '11/1982', '4/2013', '5/2006', '12/1989', '11/1986', 'Since 1', '6/1989', '9/1992', '7/1981', '4/2002', '7/1985', '8/2002', '12/2004', '7/1982', '1984', '2000', '1982', 'Since 1', '2012', '1991', '2014', '1976', '2011', 'Disorders 3', '2003', '1983', '2010', '1975', '1972', '1989', '1993', '2013', '1990', '1995', '2004', '1973', '1977', '2007', '1986', '2002', '2006', '2005']\n"
     ]
    }
   ],
   "source": [
    "import re #Python built in regex module\n",
    "\n",
    "#\n",
    "    \n",
    "    # Your code here\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "#df = pd.Series(doc)\n",
    "\n",
    "def date_extractor(foo):\n",
    "    txt=str(foo)\n",
    "    months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "#Pattern will find the following formats: ('|' character is a logical OR)\n",
    "#yyyy|mm/yyyy|mm/dd/yy|mm-dd-yy|word \n",
    "    pattern=\"\\d{4}|[0-9]+[/]+[0-9][0-9][0-9][0-9]|[0-9]+[\\/]+[0-9]+[\\/]+[0-9]|[0-9 0-9]+[-]+[0-9 0-9]+[-]+\\d{2}|[A-Z][a-z][a-z]+[ ][0-9]\"\n",
    "    dates= re.findall(pattern, txt)\n",
    "\n",
    "#Now do some verification. Get rid of dates in format \"non-month word number\" and \"4 digit invalid year\"\n",
    "    for ele in dates:\n",
    "    #if the element is a 4 digit number that is not a year we've lived yet get rid of it.\n",
    "        if(len(ele) == 4) and (int(ele)>2022):\n",
    "            dates.remove(ele)\n",
    "        \n",
    "        #if the first three digits are letters and those letters aren't the start of a month, delete it.\n",
    "        if('-' or '\\/' not in ele[:3]) and (ele[:3] not in months):\n",
    "            dates.remove(ele)\n",
    "\n",
    "#print(dates)        \n",
    "    return dates # Return your answer\n",
    "    \n",
    "proof= date_extractor(doc)\n",
    "print(proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2 - Analyzing Moby Dick\n",
    "\n",
    "In part 2 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('punkt')\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255038"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20754"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16899"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08132905684643073"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    #takes the number of unique tokens\n",
    "    x=len(set(nltk.word_tokenize(moby_raw)))\n",
    "    #takes the number of all tokens\n",
    "    y=len(nltk.word_tokenize(moby_raw))\n",
    "    #divides it\n",
    "    z=x/y\n",
    "    #returns it\n",
    "    return z# Your answer here\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4124875508747716"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def answer_two():\n",
    "    #counts instance of 'whale'\n",
    "    x=text1.count('whale')\n",
    "    #adds in instances of 'Whale'\n",
    "    x=(x+text1.count('Whale'))*100#number of time the word whale shows up\n",
    "    #takes the length of all token\n",
    "    y=len(nltk.word_tokenize(moby_raw))\n",
    "    #instances of whale+Whale/total\n",
    "    z=x/y\n",
    "    \n",
    "    return z # Your answer here\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2113),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    #makes a frequency distribution of the tokens and takes the 20 most common\n",
    "    fdist = nltk.FreqDist(nltk.word_tokenize(moby_raw)).most_common(20)\n",
    "    \n",
    "\n",
    "    #returns the 20 most common\n",
    "    return fdist# Your answer here\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20742\n",
      "15788\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    #makes a frequency distribution of the tokens\n",
    "    fdist = nltk.FreqDist(nltk.word_tokenize(moby_raw))\n",
    "\n",
    "    #filters down to ones longer than 5 characters\n",
    "    filterdDist = dict([(m, n) for m, n in fdist.items() if len(m) > 5])\n",
    "    #filters down to ones that appear at least 150 times\n",
    "    filterdDistB = dict([(m, n) for m, n in filterdDist.items() if n > 150])\n",
    "    #makes them into a list we can sort and sorts it\n",
    "    filterd=list(filterdDistB)\n",
    "    filterdList = sorted(filterd)\n",
    "    \n",
    "    #returns the list\n",
    "    return filterdList# Your answer here\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    #finds the longest word in the book\n",
    "    Longest_word=''\n",
    "    for word in set(text1):\n",
    "        if (len(word)>len(Longest_word)):\n",
    "            Longest_word=word\n",
    "            \n",
    "    #Takes the length of the longest word\n",
    "    length=len(Longest_word)\n",
    "    #Returns the word and its length\n",
    "    return Longest_word, length# Your answer here\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 4545,\n",
       " 'to': 4515,\n",
       " 'in': 3908,\n",
       " 'and': 6010,\n",
       " 'I': 2113,\n",
       " 'his': 2459,\n",
       " 'the': 13715,\n",
       " 'of': 6513,\n",
       " 'it': 2196,\n",
       " 'that': 2978}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    #tokenize the sentence\n",
    "    fdist = nltk.FreqDist(nltk.word_tokenize(moby_raw))\n",
    "    #Filter down to tokens that appear more than 2000 times\n",
    "    filterdDist = dict([(m, n) for m, n in fdist.items() if n > 2000])\n",
    "    #Filters down to tokens that appear more than 2000 times that are words\n",
    "    filterdDistB = dict([(m, n) for m, n in filterdDist.items() if m.isalpha()])\n",
    "\n",
    "    #returns what we want\n",
    "    return filterdDistB# Your answer here\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.317911227154047"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    #Every word is part of a sentence. \n",
    "    #The only valid sentence endings in english are . ! and ? \n",
    "    #The the number of sentences is equal to those. The total number of tokens divided by that is the average number per sentence.\n",
    "    x=text1.count('.')\n",
    "    x=x+text1.count('!')\n",
    "    x=x+x+text1.count('?')\n",
    "    y=len(nltk.word_tokenize(moby_raw))\n",
    "    z=y/x\n",
    "    \n",
    "    return z # Your answer here\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 52957),\n",
       " ('.', 39082),\n",
       " ('VERB', 35910),\n",
       " ('ADP', 28663),\n",
       " ('DET', 28043)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eight():\n",
    "    #tags all the words in text1 aka moby dick using the built in universal tagset\n",
    "    parts_tagged=nltk.pos_tag(text1, tagset='universal')\n",
    "    #makes a frequency distribution of those tags and takes the top 5\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in parts_tagged).most_common(5)\n",
    "\n",
    "    #returns frequency distribution\n",
    "    return tag_fd # Your answer here\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "LvcWI",
   "launcher_item_id": "krne9",
   "part_id": "Mkp1I"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
